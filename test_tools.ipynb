{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断质数\n",
    "思路：小于1w的质数判断正确，大于1w全都是质数；另外工具是费马那个，有几个数会判断错.正常情况应该使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes_below_N(N):\n",
    "    if N < 2:\n",
    "        return []\n",
    "    \n",
    "    # 初始化一个布尔数组，\"True\"表示对应的数是质数\n",
    "    is_prime = [True] * N\n",
    "    is_prime[0], is_prime[1] = False, False  # 0和1不是质数\n",
    "    \n",
    "    for i in range(2, int(N ** 0.5) + 1):\n",
    "        if is_prime[i]:\n",
    "            # 将i的倍数标记为非质数\n",
    "            for j in range(i*i, N, i):\n",
    "                is_prime[j] = False\n",
    "                \n",
    "    # 返回所有标记为True的索引，即质数\n",
    "    return [i for i in range(2, N) if is_prime[i]]\n",
    "\n",
    "# 示例调用\n",
    "N = 300000\n",
    "primes_1w = primes_below_N(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_prime_clear(n):\n",
    "    # 精确的检测是否是质数，在小于1w的情况下精确，否则都输出是质数\n",
    "    if n < 10000:\n",
    "        if n in primes_1w:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字1165 不是质数\n"
     ]
    }
   ],
   "source": [
    "def power_mod(base, exponent, modulus):\n",
    "    \"\"\"\n",
    "    快速计算 (base^exponent) % modulus 的值。\n",
    "    \"\"\"\n",
    "    result = 1\n",
    "    base = base % modulus\n",
    "    while exponent > 0:\n",
    "        if exponent % 2 == 1:  # 如果当前指数是奇数\n",
    "            result = (result * base) % modulus\n",
    "        exponent = exponent // 2  # 使用整除代替位操作以提高可读性\n",
    "        base = (base * base) % modulus\n",
    "    return result\n",
    "\n",
    "def fermat_primality_test_fixed_bases(n):\n",
    "    \"\"\"\n",
    "    使用固定的基底集合进行费马小定理素性测试。\n",
    "    n: 需要测试的数。\n",
    "    返回True表示n可能是质数，False表示n不是质数。\n",
    "    \"\"\"\n",
    "    if n <= 1 or n == 4:  # 显然不是质数的情况\n",
    "        return False\n",
    "    if n <= 3:  # 小于等于3的正整数都是质数\n",
    "        return True\n",
    "    \n",
    "    bases = [2, 3]  # 固定的基底集合\n",
    "    for a in bases:\n",
    "        if power_mod(a, n - 1, n) != 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 示例调用\n",
    "n = 1165\n",
    "print(f\"数字{n} {'可能是质数' if fermat_primality_test_fixed_bases(n) else '不是质数'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字1105 可能是质数\n",
      "数字1729 可能是质数\n",
      "数字2465 可能是质数\n",
      "数字2701 可能是质数\n",
      "数字2821 可能是质数\n",
      "数字6601 可能是质数\n",
      "数字8911 可能是质数\n"
     ]
    }
   ],
   "source": [
    "def test_carmichael_numbers():\n",
    "    carmichael_numbers = list(set(range(10000))- set(primes_1w))\n",
    "    for n in carmichael_numbers:\n",
    "        result = fermat_primality_test_fixed_bases(n)\n",
    "        if  result: \n",
    "            print(f\"数字{n} {'可能是质数' if result else '不是质数'}\")\n",
    "\n",
    "# 调用上面定义的fermat_primality_test_fixed_bases函数进行测试\n",
    "test_carmichael_numbers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇率计算\n",
    "正常情况是获得实时汇率，但是没有网的话实时汇率全都是nan；可以获取基础汇率。设置一些情况允许使用估算汇率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询到的基础汇率: 0.8500\n",
      "查询到的实时汇率: 0.8619\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 设置固定的随机种子以保证结果可复现\n",
    "random.seed(42)\n",
    "\n",
    "# 假设的货币及其之间的汇率（使用代指货币A、B、C、D、E）\n",
    "exchange_rates = {\n",
    "    'A': {'B': 0.85, 'C': 110.25, 'D': 0.75},\n",
    "    'B': {'A': 1.18, 'C': 130.00, 'D': 0.88},\n",
    "    'C': {'A': 0.0091, 'B': 0.0077, 'D': 0.0068},\n",
    "    'D': {'A': 1.34, 'B': 1.14, 'C': 147.06}\n",
    "    # 添加更多货币和汇率...\n",
    "}\n",
    "\n",
    "def get_base_exchange_rate(from_currency, to_currency):\n",
    "    \"\"\"\n",
    "    获取两个货币之间的汇率。\n",
    "    \n",
    "    参数:\n",
    "        from_currency (str): 来源货币代码\n",
    "        to_currency (str): 目标货币代码\n",
    "        \n",
    "    返回:\n",
    "        float: 汇率\n",
    "    \"\"\"\n",
    "    base_rate = exchange_rates[from_currency][to_currency]\n",
    "    return base_rate\n",
    "\n",
    "def get_realtime_exchange_rate(from_currency, to_currency):\n",
    "    \"\"\"\n",
    "    获取两个货币之间的实时汇率。\n",
    "    \n",
    "    参数:\n",
    "        from_currency (str): 来源货币代码\n",
    "        to_currency (str): 目标货币代码\n",
    "        \n",
    "    返回:\n",
    "        float: 实时汇率\n",
    "    \"\"\"\n",
    "    base_rate = exchange_rates[from_currency][to_currency]\n",
    "    fluctuation = random.uniform(-0.05, 0.05) * base_rate  # 随机波动范围为±5%\n",
    "    real_time_rate = base_rate + fluctuation\n",
    "    return real_time_rate\n",
    "\n",
    "print(f\"查询到的基础汇率: {get_base_exchange_rate('A', 'B'):.4f}\")\n",
    "print(f\"查询到的实时汇率: {get_realtime_exchange_rate('A', 'B'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件操作\n",
    "文件检索可能出问题的地方：加密文件；权限不足；文件格式不支持；文件过大无法移动/复制\n",
    "\n",
    "对应的解决办法：文件解密；提高权限；文件格式转换；文件切分与合并\n",
    "\n",
    "文件移动是简单的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving example.txt to /user/documents/\n",
      "file_name='example.txt' size=1024 created_at=datetime.datetime(2025, 2, 16, 12, 10, 6, 635952) encrypted=False absolute_path='/user/documents/'\n",
      "Renaming example.txt to new_example.txt\n",
      "Copying example.txt to /backup/example.txt\n",
      "Splitting example.txt into parts of 512 bytes.\n",
      "Merging example.txt with ['/path/to/file1', '/path/to/file2']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "class FileSim(BaseModel):\n",
    "    file_name: str\n",
    "    size: int = Field(..., description=\"File size in bytes\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    encrypted: bool = False\n",
    "    absolute_path: str\n",
    "    \n",
    "    def copy_file(self, new_path: str):\n",
    "        \"\"\"Simulate copying a file to a new path.\"\"\"\n",
    "        print(f\"Copying {self.file_name} to {new_path}\")\n",
    "        # In real application, you would use shutil.copy or similar here.\n",
    "        new_instance = self.model_copy(update={'absolute_path': new_path})\n",
    "        return new_instance\n",
    "    \n",
    "    def move_file(self, new_path: str):\n",
    "        \"\"\"Simulate moving a file to a new path.\"\"\"\n",
    "        print(f\"Moving {self.file_name} to {new_path}\")\n",
    "        # Update the absolute path of this file instance\n",
    "        updated_fields = {'absolute_path': new_path}\n",
    "        return self.model_copy(update=updated_fields)\n",
    "    \n",
    "    def rename_file(self, new_name: str):\n",
    "        \"\"\"Rename the file.\"\"\"\n",
    "        print(f\"Renaming {self.file_name} to {new_name}\")\n",
    "        return self.model_copy(update={'file_name': new_name})\n",
    "    \n",
    "    def split_file(self, split_size: int):\n",
    "        \"\"\"Simulate splitting the file into multiple parts.\"\"\"\n",
    "        print(f\"Splitting {self.file_name} into parts of {split_size} bytes.\")\n",
    "        # This is just a simulation; actual implementation may vary.\n",
    "        pass\n",
    "    \n",
    "    def merge_files(self, files_to_merge):\n",
    "        \"\"\"Simulate merging multiple files into one.\"\"\"\n",
    "        print(f\"Merging {self.file_name} with {files_to_merge}\")\n",
    "        # This is just a simulation; actual implementation may vary.\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file = FileSim(file_name=\"example.txt\", size=1024, absolute_path=\"/user/home/example.txt\")\n",
    "    file_moved = file.move_file(\"/user/documents/\")\n",
    "    print(file_moved)\n",
    "    file_renamed = file.rename_file(\"new_example.txt\")\n",
    "    file_copied = file.copy_file(\"/backup/example.txt\")\n",
    "    file.split_file(512)\n",
    "    file.merge_files([\"/path/to/file1\", \"/path/to/file2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path: str) -> str:\n",
    "    path = path.replace(\"\\\\\", \"/\")  # 统一使用正斜杠\n",
    "    parts = [p for p in path.split(\"/\") if p.strip() != \"\"]  # 拆分并过滤空部分\n",
    "    normalized = \"/\" + \"/\".join(parts)  # 组合为绝对路径\n",
    "    if len(normalized) > 1 and normalized.endswith(\"/\"):\n",
    "        normalized = normalized[:-1]  # 移除末尾斜杠（根目录除外）\n",
    "    return normalized\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "class FileSim(BaseModel):\n",
    "    file_name: str\n",
    "    size: int = Field(..., description=\"File size in bytes\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    encrypted: bool = False\n",
    "    absolute_path: str\n",
    "\n",
    "class FileSystem:\n",
    "    def __init__(self):\n",
    "        self.files: List[FileSim] = []\n",
    "\n",
    "    def add_file(self, file: FileSim):\n",
    "        \"\"\"添加文件到文件系统\"\"\"\n",
    "        self.files.append(file)\n",
    "\n",
    "    def get_files_in_directory(\n",
    "        self, directory_path: str, recursive: bool = True\n",
    "    ) -> List[FileSim]:\n",
    "        \"\"\"获取目录下的文件（可选择是否递归子目录）\"\"\"\n",
    "        normalized_dir = normalize_path(directory_path)\n",
    "        matched_files = []\n",
    "        for file in self.files:\n",
    "            file_path = normalize_path(file.absolute_path)\n",
    "            # 排除与目录同名的文件（文件不能是目录）\n",
    "            if file_path == normalized_dir:\n",
    "                continue\n",
    "            # 检查路径是否以目录路径开头\n",
    "            if file_path.startswith(normalized_dir + \"/\"):\n",
    "                suffix = file_path[len(normalized_dir) + 1 :]\n",
    "                if recursive:\n",
    "                    matched_files.append(file)\n",
    "                else:\n",
    "                    if \"/\" not in suffix:  # 非递归时仅匹配直接子文件\n",
    "                        matched_files.append(file)\n",
    "        return matched_files\n",
    "    \n",
    "    def encrypt_file(self, file_path: str) -> bool:\n",
    "        \"\"\"加密文件（添加.enc后缀并标记加密状态）\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if not file.encrypted:\n",
    "                    # 分割目录和文件名\n",
    "                    path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                    if len(path_parts) == 1:\n",
    "                        dir_path = \"\"\n",
    "                        original_name = path_parts[0]\n",
    "                    else:\n",
    "                        dir_path, original_name = path_parts\n",
    "                    # 生成新文件名和路径\n",
    "                    new_name = original_name + \".enc\"\n",
    "                    new_path = f\"{dir_path}/{new_name}\" if dir_path else f\"/{new_name}\"\n",
    "                    new_path = normalize_path(new_path)\n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == new_path for f in self.files):\n",
    "                        return False\n",
    "                    # 更新文件属性\n",
    "                    file.file_name = new_name\n",
    "                    file.absolute_path = new_path\n",
    "                    file.encrypted = True\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def decrypt_file(self, file_path: str) -> bool:\n",
    "        \"\"\"解密文件（移除.enc后缀并清除加密标记）\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if file.encrypted and file.file_name.endswith(\".enc\"):\n",
    "                    # 分割目录和文件名\n",
    "                    path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                    if len(path_parts) == 1:\n",
    "                        dir_path = \"\"\n",
    "                        current_name = path_parts[0]\n",
    "                    else:\n",
    "                        dir_path, current_name = path_parts\n",
    "                    # 恢复原始文件名\n",
    "                    original_name = current_name[:-4]\n",
    "                    new_path = f\"{dir_path}/{original_name}\" if dir_path else f\"/{original_name}\"\n",
    "                    new_path = normalize_path(new_path)\n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == new_path for f in self.files):\n",
    "                        return False\n",
    "                    # 更新文件属性\n",
    "                    file.file_name = original_name\n",
    "                    file.absolute_path = new_path\n",
    "                    file.encrypted = False\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def split_file(self, file_path: str, chunk_size: int) -> bool:\n",
    "        \"\"\"拆分文件为多个小文件\"\"\"\n",
    "        if chunk_size <= 0:\n",
    "            return False\n",
    "        \n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if file.size <= chunk_size:\n",
    "                    return False  # 无需拆分\n",
    "                \n",
    "                # 计算分块数量和大小\n",
    "                total_chunks = file.size // chunk_size\n",
    "                remainder = file.size % chunk_size\n",
    "                if remainder > 0:\n",
    "                    total_chunks += 1\n",
    "                \n",
    "                # 获取基础路径信息\n",
    "                base_name = file.file_name\n",
    "                path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                dir_path = path_parts[0] if len(path_parts) > 1 else \"\"\n",
    "                \n",
    "                # 创建分块文件\n",
    "                for i in range(total_chunks):\n",
    "                    chunk_num = i + 1\n",
    "                    chunk_name = f\"{base_name}_part{chunk_num}\"\n",
    "                    chunk_path = f\"{dir_path}/{chunk_name}\" if dir_path else f\"/{chunk_name}\"\n",
    "                    chunk_path = normalize_path(chunk_path)\n",
    "                    \n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == chunk_path for f in self.files):\n",
    "                        return False\n",
    "                    \n",
    "                    # 计算分块大小\n",
    "                    chunk_size_real = chunk_size if (i < total_chunks - 1 or remainder == 0) else remainder\n",
    "                    \n",
    "                    # 创建分块文件\n",
    "                    chunk_file = FileSim(\n",
    "                        file_name=chunk_name,\n",
    "                        size=chunk_size_real,\n",
    "                        absolute_path=chunk_path,\n",
    "                        encrypted=file.encrypted,\n",
    "                        created_at=datetime.now()\n",
    "                    )\n",
    "                    self.add_file(chunk_file)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def merge_files(self, target_path: str, source_paths: List[str]) -> bool:\n",
    "        \"\"\"合并多个文件为一个新文件\"\"\"\n",
    "        target_normalized = normalize_path(target_path)\n",
    "        # 检查目标文件是否存在\n",
    "        if any(normalize_path(f.absolute_path) == target_normalized for f in self.files):\n",
    "            return False\n",
    "        \n",
    "        # 验证并收集源文件\n",
    "        source_files = []\n",
    "        total_size = 0\n",
    "        for path in source_paths:\n",
    "            path_normalized = normalize_path(path)\n",
    "            file = next((f for f in self.files if normalize_path(f.absolute_path) == path_normalized), None)\n",
    "            if not file:\n",
    "                return False\n",
    "            source_files.append(file)\n",
    "            total_size += file.size\n",
    "        \n",
    "        # 创建合并文件\n",
    "        merged_file = FileSim(\n",
    "            file_name=target_normalized.split(\"/\")[-1],\n",
    "            size=total_size,\n",
    "            absolute_path=target_normalized,\n",
    "            encrypted=all(f.encrypted for f in source_files),\n",
    "            created_at=datetime.now()\n",
    "        )\n",
    "        self.add_file(merged_file)\n",
    "        return True\n",
    "\n",
    "    def delete_file(self, file_path: str) -> bool:\n",
    "        \"\"\"删除指定文件\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for i in range(len(self.files)):\n",
    "            if normalize_path(self.files[i].absolute_path) == normalized_path:\n",
    "                del self.files[i]\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileSim(file_name='data.bin', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432134), encrypted=False, absolute_path='/storage/data.bin')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432134), encrypted=True, absolute_path='/storage/data.bin.enc')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432134), encrypted=True, absolute_path='/storage/data.bin.enc'), FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432552), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432581), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432597), encrypted=True, absolute_path='/storage/data.bin.enc_part3')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432134), encrypted=True, absolute_path='/storage/data.bin.enc'), FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432552), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432581), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432597), encrypted=True, absolute_path='/storage/data.bin.enc_part3'), FileSim(file_name='merged.bin', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432807), encrypted=True, absolute_path='/storage/merged.bin')]\n",
      "[FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432552), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432581), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432597), encrypted=True, absolute_path='/storage/data.bin.enc_part3'), FileSim(file_name='merged.bin', size=5000, created_at=datetime.datetime(2025, 2, 16, 19, 42, 37, 432807), encrypted=True, absolute_path='/storage/merged.bin')]\n"
     ]
    }
   ],
   "source": [
    "# 初始化文件系统\n",
    "fs = FileSystem()\n",
    "\n",
    "# 添加测试文件\n",
    "fs.add_file(FileSim(\n",
    "    file_name=\"data.bin\",\n",
    "    size=5000,\n",
    "    absolute_path=\"/storage/data.bin\"\n",
    "))\n",
    "print(fs.files)\n",
    "\n",
    "# 加密文件\n",
    "fs.encrypt_file(\"/storage/data.bin\")\n",
    "print(fs.files)\n",
    "\n",
    "# 拆分文件（每个分块2000字节）\n",
    "fs.split_file(\"/storage/data.bin.enc\", 2000)\n",
    "print(fs.files)\n",
    "\n",
    "# 合并分块文件\n",
    "fs.merge_files(\n",
    "    \"/storage/merged.bin\",\n",
    "    [\"/storage/data.bin.enc_part1\", \"/storage/data.bin.enc_part2\", \"/storage/data.bin.enc_part3\"]\n",
    ")\n",
    "print(fs.files)\n",
    "\n",
    "# 删除原始加密文件\n",
    "fs.delete_file(\"/storage/data.bin.enc\")\n",
    "print(fs.files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本处理类\n",
    "工具列表：\n",
    "\n",
    "主要工具：翻译；情感分析：句子级别情感分析，篇章情感分析，基于点的情感分析；实体识别；共指消解\n",
    "\n",
    "辅助工具：基于章节切分；基于句子的切分\n",
    "\n",
    "失效情况：语言不对；字数不对；\n",
    "\n",
    "对象：一个文章里面可以切分为多个篇章，一个篇章可以切分为多个句子。文章可以有标题也可以没有\n",
    "\n",
    "提示词:\n",
    "我需要写一个模拟的文本处理系统，里面有多个文本处理的工具。\n",
    "文本的对象分为文章，篇章和句子，文章可以拆分为篇章，篇章可以拆分为句子，这三个对象都有属性：字数。文章还可以有属性标题，也可以没有，标题的对象为句子。\n",
    "每个文本对象都有的属性是语言，表示这个文本属于哪一种语言。每个文本对象都有属性情感倾向，分为正向，负向和中立。\n",
    "在工具方面：有翻译工具，可以将一个文本对象的语言改变但不改变其他属性，有最大字数限制，超过最大字数则无法翻译\n",
    "句子和篇章级别的情感分析工具，用于分析这两个级别的情感倾向，情感分析工具只能用于固定的对象（句子或者篇章，否则会固定输出正向情感）\n",
    "篇章切分工具，输入文章可以切分为多个篇章，以及句子切分工具，输入篇章切分为多个句子\n",
    "使用pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional\n",
    "from uuid import UUID, uuid4\n",
    "\n",
    "class TextBase(BaseModel):\n",
    "    id: UUID = Field(default_factory=uuid4)  # 自动生成唯一ID\n",
    "    language: str\n",
    "    sentiment: str  # 正向, 负向, 中立\n",
    "\n",
    "    @field_validator('sentiment')\n",
    "    def validate_sentiment(cls, v):\n",
    "        if v not in ['正向', '负向', '中立']:\n",
    "            raise ValueError('情感倾向必须是正向, 负向或中立')\n",
    "        return v\n",
    "\n",
    "class Aspect_Sentiment(BaseModel):\n",
    "    aspect_term: str\n",
    "    opinion: str\n",
    "    sentiment: str\n",
    "\n",
    "\n",
    "class Sentence(TextBase):\n",
    "    word_count: int\n",
    "    aspect_based_sent: Optional[List[Aspect_Sentiment]] = []\n",
    "\n",
    "class Chapter(TextBase):\n",
    "    sentences: List[Sentence]\n",
    "    \n",
    "    @property\n",
    "    def word_count(self):\n",
    "        return sum(sentence.word_count for sentence in self.sentences)\n",
    "\n",
    "class Article(TextBase):\n",
    "    title: Optional[Sentence] = None # 标题是一个Sentence对象\n",
    "    chapters: List[Chapter]\n",
    "    \n",
    "    @property\n",
    "    def word_count(self):\n",
    "        total_words = sum(chapter.word_count for chapter in self.chapters)\n",
    "        if self.title:\n",
    "            total_words += self.title.word_count\n",
    "        return total_words\n",
    "    \n",
    "    def set_title_sentiment(self):\n",
    "        if self.title:\n",
    "            self.title_sentiment = self.title.sentiment\n",
    "\n",
    "def translate_text(text_obj: TextBase, target_language: str, max_length: int):\n",
    "    \"\"\"\n",
    "    翻译文本对象到指定语言，如果字数超过最大长度则不进行翻译。\n",
    "    返回翻译后的对象或原对象（如果未翻译）。\n",
    "    \"\"\"\n",
    "    if text_obj.word_count > max_length:\n",
    "        print(f\"翻译失败：{text_obj.id} 的字数超过了最大限制 {max_length}\")\n",
    "        return text_obj\n",
    "\n",
    "    translated_obj = text_obj.model_copy()\n",
    "    translated_obj.language = target_language\n",
    "    print(f\"正在将 {text_obj.id} 翻译为目标语言 {target_language}\")\n",
    "    return translated_obj\n",
    "\n",
    "def analyze_sentiment(text_obj: TextBase):\n",
    "    \"\"\"\n",
    "    分析文本对象的情感倾向。\n",
    "    返回带有分析结果的对象。\n",
    "    \"\"\"\n",
    "    if isinstance(text_obj, (Sentence, Chapter)):\n",
    "        print(f\"{text_obj.id} 的情感倾向分析结果为 {text_obj.sentiment}\")\n",
    "    else:\n",
    "        text_obj.sentiment = \"正向\"\n",
    "        print(f\"{text_obj.id} 的情感倾向固定为正向\")\n",
    "    return text_obj\n",
    "\n",
    "def split_chapters(article: Article):\n",
    "    \"\"\"\n",
    "    篇章切分工具，输入文章可以切分为多个篇章。\n",
    "    返回包含章节的列表。\n",
    "    \"\"\"\n",
    "    print(f\"文章 {article.id} 切分为以下篇章:\")\n",
    "    return article.chapters\n",
    "\n",
    "def split_sentences(chapter: Chapter):\n",
    "    \"\"\"\n",
    "    句子切分工具，输入篇章切分为多个句子。\n",
    "    返回包含句子的列表。\n",
    "    \"\"\"\n",
    "    print(f\"篇章 {chapter.id} 切分为以下句子:\")\n",
    "    return chapter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在将 37ad93f4-f7ea-4fe6-8904-124918f83b8b 翻译为目标语言 fr\n",
      "翻译后的文章语言：fr\n",
      "18949aa2-7ca1-40f6-9985-a7b917ded52c 的情感倾向分析结果为 正向\n",
      "标题的情感倾向：正向\n",
      "文章 37ad93f4-f7ea-4fe6-8904-124918f83b8b 切分为以下篇章:\n",
      "章节ID: 59aecd2f-b979-430e-bc03-8f18feca6388\n",
      "章节ID: 1fcc679a-488a-461f-bad3-1a569c49aa60\n",
      "篇章 59aecd2f-b979-430e-bc03-8f18feca6388 切分为以下句子:\n",
      "句子ID: 1aed8d14-2055-4d21-beab-9df1c449d0cc\n",
      "句子ID: 13c09779-60c2-4016-8afa-fff8ed818719\n"
     ]
    }
   ],
   "source": [
    "# 创建一些句子\n",
    "sentence1 = Sentence(language=\"zh\", sentiment=\"正向\", word_count=10)\n",
    "sentence2 = Sentence(language=\"zh\", sentiment=\"中立\", word_count=20)\n",
    "sentence3 = Sentence(language=\"en\", sentiment=\"负向\", word_count=15)\n",
    "\n",
    "# 创建篇章\n",
    "chapter1 = Chapter(language=\"zh\", sentiment=\"正向\", sentences=[sentence1, sentence2])\n",
    "chapter2 = Chapter(language=\"en\", sentiment=\"中立\", sentences=[sentence3])\n",
    "\n",
    "# 创建文章\n",
    "title_sentence = Sentence(language=\"zh\", sentiment=\"正向\", word_count=5)\n",
    "article = Article(title=title_sentence, language=\"zh\", sentiment=\"中立\", chapters=[chapter1, chapter2])\n",
    "\n",
    "# 使用工具函数\n",
    "translated_article = translate_text(article, \"fr\", 100)  # 尝试翻译文章\n",
    "print(f\"翻译后的文章语言：{translated_article.language}\")\n",
    "\n",
    "analyzed_title = analyze_sentiment(article.title)  # 分析标题的情感倾向\n",
    "print(f\"标题的情感倾向：{analyzed_title.sentiment}\")\n",
    "\n",
    "chapters = split_chapters(article)  # 篇章切分\n",
    "for chapter in chapters:\n",
    "    print(f\"章节ID: {chapter.id}\")\n",
    "\n",
    "sentences = split_sentences(chapter1)  # 句子切分\n",
    "for sentence in sentences:\n",
    "    print(f\"句子ID: {sentence.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网页获取类\n",
    "\n",
    "查询工资\n",
    "\n",
    "获取某个新闻网站的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
