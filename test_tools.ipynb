{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断质数\n",
    "思路：小于1w的质数判断正确，大于1w全都是质数；另外工具是费马那个，有几个数会判断错.正常情况应该使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes_below_N(N):\n",
    "    if N < 2:\n",
    "        return []\n",
    "    \n",
    "    # 初始化一个布尔数组，\"True\"表示对应的数是质数\n",
    "    is_prime = [True] * N\n",
    "    is_prime[0], is_prime[1] = False, False  # 0和1不是质数\n",
    "    \n",
    "    for i in range(2, int(N ** 0.5) + 1):\n",
    "        if is_prime[i]:\n",
    "            # 将i的倍数标记为非质数\n",
    "            for j in range(i*i, N, i):\n",
    "                is_prime[j] = False\n",
    "                \n",
    "    # 返回所有标记为True的索引，即质数\n",
    "    return [i for i in range(2, N) if is_prime[i]]\n",
    "\n",
    "# 示例调用\n",
    "N = 300000\n",
    "primes_1w = primes_below_N(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_prime_clear(n):\n",
    "    # 精确的检测是否是质数，在小于1w的情况下精确，否则都输出是质数\n",
    "    if n < 10000:\n",
    "        if n in primes_1w:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字1165 不是质数\n"
     ]
    }
   ],
   "source": [
    "def power_mod(base, exponent, modulus):\n",
    "    \"\"\"\n",
    "    快速计算 (base^exponent) % modulus 的值。\n",
    "    \"\"\"\n",
    "    result = 1\n",
    "    base = base % modulus\n",
    "    while exponent > 0:\n",
    "        if exponent % 2 == 1:  # 如果当前指数是奇数\n",
    "            result = (result * base) % modulus\n",
    "        exponent = exponent // 2  # 使用整除代替位操作以提高可读性\n",
    "        base = (base * base) % modulus\n",
    "    return result\n",
    "\n",
    "def fermat_primality_test_fixed_bases(n):\n",
    "    \"\"\"\n",
    "    使用固定的基底集合进行费马小定理素性测试。\n",
    "    n: 需要测试的数。\n",
    "    返回True表示n可能是质数，False表示n不是质数。\n",
    "    \"\"\"\n",
    "    if n <= 1 or n == 4:  # 显然不是质数的情况\n",
    "        return False\n",
    "    if n <= 3:  # 小于等于3的正整数都是质数\n",
    "        return True\n",
    "    \n",
    "    bases = [2, 3]  # 固定的基底集合\n",
    "    for a in bases:\n",
    "        if power_mod(a, n - 1, n) != 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 示例调用\n",
    "n = 1165\n",
    "print(f\"数字{n} {'可能是质数' if fermat_primality_test_fixed_bases(n) else '不是质数'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字1105 可能是质数\n",
      "数字1729 可能是质数\n",
      "数字2465 可能是质数\n",
      "数字2701 可能是质数\n",
      "数字2821 可能是质数\n",
      "数字6601 可能是质数\n",
      "数字8911 可能是质数\n"
     ]
    }
   ],
   "source": [
    "def test_carmichael_numbers():\n",
    "    carmichael_numbers = list(set(range(10000))- set(primes_1w))\n",
    "    for n in carmichael_numbers:\n",
    "        result = fermat_primality_test_fixed_bases(n)\n",
    "        if  result: \n",
    "            print(f\"数字{n} {'可能是质数' if result else '不是质数'}\")\n",
    "\n",
    "# 调用上面定义的fermat_primality_test_fixed_bases函数进行测试\n",
    "test_carmichael_numbers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汇率计算\n",
    "正常情况是获得实时汇率，但是没有网的话实时汇率全都是nan；可以获取基础汇率。设置一些情况允许使用估算汇率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询到的基础汇率: 0.8500\n",
      "查询到的实时汇率: 0.8619\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 设置固定的随机种子以保证结果可复现\n",
    "random.seed(42)\n",
    "\n",
    "# 假设的货币及其之间的汇率（使用代指货币A、B、C、D、E）\n",
    "exchange_rates = {\n",
    "    'A': {'B': 0.85, 'C': 110.25, 'D': 0.75},\n",
    "    'B': {'A': 1.18, 'C': 130.00, 'D': 0.88},\n",
    "    'C': {'A': 0.0091, 'B': 0.0077, 'D': 0.0068},\n",
    "    'D': {'A': 1.34, 'B': 1.14, 'C': 147.06}\n",
    "    # 添加更多货币和汇率...\n",
    "}\n",
    "\n",
    "def get_base_exchange_rate(from_currency, to_currency):\n",
    "    \"\"\"\n",
    "    获取两个货币之间的汇率。\n",
    "    \n",
    "    参数:\n",
    "        from_currency (str): 来源货币代码\n",
    "        to_currency (str): 目标货币代码\n",
    "        \n",
    "    返回:\n",
    "        float: 汇率\n",
    "    \"\"\"\n",
    "    base_rate = exchange_rates[from_currency][to_currency]\n",
    "    return base_rate\n",
    "\n",
    "def get_realtime_exchange_rate(from_currency, to_currency):\n",
    "    \"\"\"\n",
    "    获取两个货币之间的实时汇率。\n",
    "    \n",
    "    参数:\n",
    "        from_currency (str): 来源货币代码\n",
    "        to_currency (str): 目标货币代码\n",
    "        \n",
    "    返回:\n",
    "        float: 实时汇率\n",
    "    \"\"\"\n",
    "    base_rate = exchange_rates[from_currency][to_currency]\n",
    "    fluctuation = random.uniform(-0.05, 0.05) * base_rate  # 随机波动范围为±5%\n",
    "    real_time_rate = base_rate + fluctuation\n",
    "    return real_time_rate\n",
    "\n",
    "print(f\"查询到的基础汇率: {get_base_exchange_rate('A', 'B'):.4f}\")\n",
    "print(f\"查询到的实时汇率: {get_realtime_exchange_rate('A', 'B'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件操作\n",
    "文件检索可能出问题的地方：加密文件；权限不足；文件格式不支持；文件过大无法移动/复制\n",
    "\n",
    "对应的解决办法：文件解密；提高权限；文件格式转换；文件切分与合并\n",
    "\n",
    "文件移动是简单的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving example.txt to /user/documents/\n",
      "file_name='example.txt' size=1024 created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 593438) encrypted=False absolute_path='/user/documents/'\n",
      "Renaming example.txt to new_example.txt\n",
      "Copying example.txt to /backup/example.txt\n",
      "Splitting example.txt into parts of 512 bytes.\n",
      "Merging example.txt with ['/path/to/file1', '/path/to/file2']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "class FileSim(BaseModel):\n",
    "    file_name: str\n",
    "    size: int = Field(..., description=\"File size in bytes\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    encrypted: bool = False\n",
    "    absolute_path: str\n",
    "    \n",
    "    def copy_file(self, new_path: str):\n",
    "        \"\"\"Simulate copying a file to a new path.\"\"\"\n",
    "        print(f\"Copying {self.file_name} to {new_path}\")\n",
    "        # In real application, you would use shutil.copy or similar here.\n",
    "        new_instance = self.model_copy(update={'absolute_path': new_path})\n",
    "        return new_instance\n",
    "    \n",
    "    def move_file(self, new_path: str):\n",
    "        \"\"\"Simulate moving a file to a new path.\"\"\"\n",
    "        print(f\"Moving {self.file_name} to {new_path}\")\n",
    "        # Update the absolute path of this file instance\n",
    "        updated_fields = {'absolute_path': new_path}\n",
    "        return self.model_copy(update=updated_fields)\n",
    "    \n",
    "    def rename_file(self, new_name: str):\n",
    "        \"\"\"Rename the file.\"\"\"\n",
    "        print(f\"Renaming {self.file_name} to {new_name}\")\n",
    "        return self.model_copy(update={'file_name': new_name})\n",
    "    \n",
    "    def split_file(self, split_size: int):\n",
    "        \"\"\"Simulate splitting the file into multiple parts.\"\"\"\n",
    "        print(f\"Splitting {self.file_name} into parts of {split_size} bytes.\")\n",
    "        # This is just a simulation; actual implementation may vary.\n",
    "        pass\n",
    "    \n",
    "    def merge_files(self, files_to_merge):\n",
    "        \"\"\"Simulate merging multiple files into one.\"\"\"\n",
    "        print(f\"Merging {self.file_name} with {files_to_merge}\")\n",
    "        # This is just a simulation; actual implementation may vary.\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file = FileSim(file_name=\"example.txt\", size=1024, absolute_path=\"/user/home/example.txt\")\n",
    "    file_moved = file.move_file(\"/user/documents/\")\n",
    "    print(file_moved)\n",
    "    file_renamed = file.rename_file(\"new_example.txt\")\n",
    "    file_copied = file.copy_file(\"/backup/example.txt\")\n",
    "    file.split_file(512)\n",
    "    file.merge_files([\"/path/to/file1\", \"/path/to/file2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path: str) -> str:\n",
    "    path = path.replace(\"\\\\\", \"/\")  # 统一使用正斜杠\n",
    "    parts = [p for p in path.split(\"/\") if p.strip() != \"\"]  # 拆分并过滤空部分\n",
    "    normalized = \"/\" + \"/\".join(parts)  # 组合为绝对路径\n",
    "    if len(normalized) > 1 and normalized.endswith(\"/\"):\n",
    "        normalized = normalized[:-1]  # 移除末尾斜杠（根目录除外）\n",
    "    return normalized\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "class FileSim(BaseModel):\n",
    "    file_name: str\n",
    "    size: int = Field(..., description=\"File size in bytes\")\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    encrypted: bool = False\n",
    "    absolute_path: str\n",
    "\n",
    "class FileSystem:\n",
    "    def __init__(self):\n",
    "        self.files: List[FileSim] = []\n",
    "\n",
    "    def add_file(self, file: FileSim):\n",
    "        \"\"\"添加文件到文件系统\"\"\"\n",
    "        self.files.append(file)\n",
    "\n",
    "    def get_files_in_directory(\n",
    "        self, directory_path: str, recursive: bool = True\n",
    "    ) -> List[FileSim]:\n",
    "        \"\"\"获取目录下的文件（可选择是否递归子目录）\"\"\"\n",
    "        normalized_dir = normalize_path(directory_path)\n",
    "        matched_files = []\n",
    "        for file in self.files:\n",
    "            file_path = normalize_path(file.absolute_path)\n",
    "            # 排除与目录同名的文件（文件不能是目录）\n",
    "            if file_path == normalized_dir:\n",
    "                continue\n",
    "            # 检查路径是否以目录路径开头\n",
    "            if file_path.startswith(normalized_dir + \"/\"):\n",
    "                suffix = file_path[len(normalized_dir) + 1 :]\n",
    "                if recursive:\n",
    "                    matched_files.append(file)\n",
    "                else:\n",
    "                    if \"/\" not in suffix:  # 非递归时仅匹配直接子文件\n",
    "                        matched_files.append(file)\n",
    "        return matched_files\n",
    "    \n",
    "    def encrypt_file(self, file_path: str) -> bool:\n",
    "        \"\"\"加密文件（添加.enc后缀并标记加密状态）\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if not file.encrypted:\n",
    "                    # 分割目录和文件名\n",
    "                    path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                    if len(path_parts) == 1:\n",
    "                        dir_path = \"\"\n",
    "                        original_name = path_parts[0]\n",
    "                    else:\n",
    "                        dir_path, original_name = path_parts\n",
    "                    # 生成新文件名和路径\n",
    "                    new_name = original_name + \".enc\"\n",
    "                    new_path = f\"{dir_path}/{new_name}\" if dir_path else f\"/{new_name}\"\n",
    "                    new_path = normalize_path(new_path)\n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == new_path for f in self.files):\n",
    "                        return False\n",
    "                    # 更新文件属性\n",
    "                    file.file_name = new_name\n",
    "                    file.absolute_path = new_path\n",
    "                    file.encrypted = True\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def decrypt_file(self, file_path: str) -> bool:\n",
    "        \"\"\"解密文件（移除.enc后缀并清除加密标记）\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if file.encrypted and file.file_name.endswith(\".enc\"):\n",
    "                    # 分割目录和文件名\n",
    "                    path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                    if len(path_parts) == 1:\n",
    "                        dir_path = \"\"\n",
    "                        current_name = path_parts[0]\n",
    "                    else:\n",
    "                        dir_path, current_name = path_parts\n",
    "                    # 恢复原始文件名\n",
    "                    original_name = current_name[:-4]\n",
    "                    new_path = f\"{dir_path}/{original_name}\" if dir_path else f\"/{original_name}\"\n",
    "                    new_path = normalize_path(new_path)\n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == new_path for f in self.files):\n",
    "                        return False\n",
    "                    # 更新文件属性\n",
    "                    file.file_name = original_name\n",
    "                    file.absolute_path = new_path\n",
    "                    file.encrypted = False\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def split_file(self, file_path: str, chunk_size: int) -> bool:\n",
    "        \"\"\"拆分文件为多个小文件\"\"\"\n",
    "        if chunk_size <= 0:\n",
    "            return False\n",
    "        \n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for file in self.files:\n",
    "            if normalize_path(file.absolute_path) == normalized_path:\n",
    "                if file.size <= chunk_size:\n",
    "                    return False  # 无需拆分\n",
    "                \n",
    "                # 计算分块数量和大小\n",
    "                total_chunks = file.size // chunk_size\n",
    "                remainder = file.size % chunk_size\n",
    "                if remainder > 0:\n",
    "                    total_chunks += 1\n",
    "                \n",
    "                # 获取基础路径信息\n",
    "                base_name = file.file_name\n",
    "                path_parts = file.absolute_path.rsplit(\"/\", 1)\n",
    "                dir_path = path_parts[0] if len(path_parts) > 1 else \"\"\n",
    "                \n",
    "                # 创建分块文件\n",
    "                for i in range(total_chunks):\n",
    "                    chunk_num = i + 1\n",
    "                    chunk_name = f\"{base_name}_part{chunk_num}\"\n",
    "                    chunk_path = f\"{dir_path}/{chunk_name}\" if dir_path else f\"/{chunk_name}\"\n",
    "                    chunk_path = normalize_path(chunk_path)\n",
    "                    \n",
    "                    # 检查路径冲突\n",
    "                    if any(normalize_path(f.absolute_path) == chunk_path for f in self.files):\n",
    "                        return False\n",
    "                    \n",
    "                    # 计算分块大小\n",
    "                    chunk_size_real = chunk_size if (i < total_chunks - 1 or remainder == 0) else remainder\n",
    "                    \n",
    "                    # 创建分块文件\n",
    "                    chunk_file = FileSim(\n",
    "                        file_name=chunk_name,\n",
    "                        size=chunk_size_real,\n",
    "                        absolute_path=chunk_path,\n",
    "                        encrypted=file.encrypted,\n",
    "                        created_at=datetime.now()\n",
    "                    )\n",
    "                    self.add_file(chunk_file)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def merge_files(self, target_path: str, source_paths: List[str]) -> bool:\n",
    "        \"\"\"合并多个文件为一个新文件\"\"\"\n",
    "        target_normalized = normalize_path(target_path)\n",
    "        # 检查目标文件是否存在\n",
    "        if any(normalize_path(f.absolute_path) == target_normalized for f in self.files):\n",
    "            return False\n",
    "        \n",
    "        # 验证并收集源文件\n",
    "        source_files = []\n",
    "        total_size = 0\n",
    "        for path in source_paths:\n",
    "            path_normalized = normalize_path(path)\n",
    "            file = next((f for f in self.files if normalize_path(f.absolute_path) == path_normalized), None)\n",
    "            if not file:\n",
    "                return False\n",
    "            source_files.append(file)\n",
    "            total_size += file.size\n",
    "        \n",
    "        # 创建合并文件\n",
    "        merged_file = FileSim(\n",
    "            file_name=target_normalized.split(\"/\")[-1],\n",
    "            size=total_size,\n",
    "            absolute_path=target_normalized,\n",
    "            encrypted=all(f.encrypted for f in source_files),\n",
    "            created_at=datetime.now()\n",
    "        )\n",
    "        self.add_file(merged_file)\n",
    "        return True\n",
    "\n",
    "    def delete_file(self, file_path: str) -> bool:\n",
    "        \"\"\"删除指定文件\"\"\"\n",
    "        normalized_path = normalize_path(file_path)\n",
    "        for i in range(len(self.files)):\n",
    "            if normalize_path(self.files[i].absolute_path) == normalized_path:\n",
    "                del self.files[i]\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileSim(file_name='data.bin', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623012), encrypted=False, absolute_path='/storage/data.bin')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623012), encrypted=True, absolute_path='/storage/data.bin.enc')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623012), encrypted=True, absolute_path='/storage/data.bin.enc'), FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623117), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623124), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623129), encrypted=True, absolute_path='/storage/data.bin.enc_part3')]\n",
      "[FileSim(file_name='data.bin.enc', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623012), encrypted=True, absolute_path='/storage/data.bin.enc'), FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623117), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623124), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623129), encrypted=True, absolute_path='/storage/data.bin.enc_part3'), FileSim(file_name='merged.bin', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623179), encrypted=True, absolute_path='/storage/merged.bin')]\n",
      "[FileSim(file_name='data.bin.enc_part1', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623117), encrypted=True, absolute_path='/storage/data.bin.enc_part1'), FileSim(file_name='data.bin.enc_part2', size=2000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623124), encrypted=True, absolute_path='/storage/data.bin.enc_part2'), FileSim(file_name='data.bin.enc_part3', size=1000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623129), encrypted=True, absolute_path='/storage/data.bin.enc_part3'), FileSim(file_name='merged.bin', size=5000, created_at=datetime.datetime(2025, 2, 19, 18, 5, 28, 623179), encrypted=True, absolute_path='/storage/merged.bin')]\n"
     ]
    }
   ],
   "source": [
    "# 初始化文件系统\n",
    "fs = FileSystem()\n",
    "\n",
    "# 添加测试文件\n",
    "fs.add_file(FileSim(\n",
    "    file_name=\"data.bin\",\n",
    "    size=5000,\n",
    "    absolute_path=\"/storage/data.bin\"\n",
    "))\n",
    "print(fs.files)\n",
    "\n",
    "# 加密文件\n",
    "fs.encrypt_file(\"/storage/data.bin\")\n",
    "print(fs.files)\n",
    "\n",
    "# 拆分文件（每个分块2000字节）\n",
    "fs.split_file(\"/storage/data.bin.enc\", 2000)\n",
    "print(fs.files)\n",
    "\n",
    "# 合并分块文件\n",
    "fs.merge_files(\n",
    "    \"/storage/merged.bin\",\n",
    "    [\"/storage/data.bin.enc_part1\", \"/storage/data.bin.enc_part2\", \"/storage/data.bin.enc_part3\"]\n",
    ")\n",
    "print(fs.files)\n",
    "\n",
    "# 删除原始加密文件\n",
    "fs.delete_file(\"/storage/data.bin.enc\")\n",
    "print(fs.files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本处理类\n",
    "工具列表：\n",
    "\n",
    "主要工具：翻译；情感分析：句子级别情感分析，篇章情感分析，基于点的情感分析；实体识别；共指消解\n",
    "\n",
    "辅助工具：基于章节切分；基于句子的切分\n",
    "\n",
    "失效情况：语言不对；字数不对；\n",
    "\n",
    "对象：一个文章里面可以切分为多个篇章，一个篇章可以切分为多个句子。文章可以有标题也可以没有\n",
    "\n",
    "提示词:\n",
    "我需要写一个模拟的文本处理系统，里面有多个文本处理的工具。\n",
    "文本的对象分为文章，篇章和句子，文章可以拆分为篇章，篇章可以拆分为句子，这三个对象都有属性：字数。文章还可以有属性标题，也可以没有，标题的对象为句子。\n",
    "每个文本对象都有的属性是语言，表示这个文本属于哪一种语言。每个文本对象都有属性情感倾向，分为正向，负向和中立。\n",
    "在工具方面：有翻译工具，可以将一个文本对象的语言改变但不改变其他属性，有最大字数限制，超过最大字数则无法翻译\n",
    "句子和篇章级别的情感分析工具，用于分析这两个级别的情感倾向，情感分析工具只能用于固定的对象（句子或者篇章，否则会固定输出正向情感）\n",
    "篇章切分工具，输入文章可以切分为多个篇章，以及句子切分工具，输入篇章切分为多个句子\n",
    "使用pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from uuid import UUID, uuid4\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "import random\n",
    "\n",
    "def random_senti():\n",
    "    return random.choice([\"pos\", \"neg\", \"neu\"])\n",
    "\n",
    "class Aspect_Sentiment(BaseModel):\n",
    "    aspect_term: str\n",
    "    opinion: str\n",
    "    sentiment: str\n",
    "\n",
    "class TextBase(BaseModel):\n",
    "    id: UUID = Field(default_factory=uuid4)\n",
    "    content: List[UUID]\n",
    "    language: str\n",
    "    sentiment: str = Field(default_factory=random_senti)  # 自动生成随机情感\n",
    "\n",
    "    @field_validator('sentiment')\n",
    "    def validate_sentiment(cls, v):\n",
    "        if v not in ['pos', 'neg', 'neu']:\n",
    "            raise ValueError('情感倾向必须是pos, neg, neu')\n",
    "        return v\n",
    "\n",
    "class Sentence(TextBase):\n",
    "    word_count: Optional[int] = None  # 占位值，实际由验证器计算\n",
    "    aspect_based_sent: Optional[List[Aspect_Sentiment]] = None\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def adjust_word_count(self):\n",
    "        if self.word_count is None:\n",
    "            self.word_count = random.randint(10, 20)\n",
    "        return self\n",
    "\n",
    "class Chapter(TextBase):\n",
    "    sentences: List[Sentence]\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def assemble_content(cls, data: dict) -> dict:\n",
    "        \"\"\"自动聚合句子的content并去重\"\"\"\n",
    "        sentences = data.get('sentences', [])\n",
    "        content = []\n",
    "        for sentence in sentences:\n",
    "            if isinstance(sentence, Sentence):\n",
    "                content.extend(sentence.content)\n",
    "            else:\n",
    "                content.extend(sentence[\"content\"])\n",
    "\n",
    "        data['content'] = list(set(content))\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def word_count(self):\n",
    "        return sum(sentence.word_count for sentence in self.sentences)\n",
    "\n",
    "class Article(TextBase):\n",
    "    title: Optional[Sentence] = None\n",
    "    chapters: List[Chapter]\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def assemble_content(cls, data: dict) -> dict:\n",
    "        \"\"\"聚合标题和章节的content并去重\"\"\"\n",
    "        content = []\n",
    "        if title := data.get('title'):\n",
    "            if isinstance(title, Sentence):\n",
    "                content.extend(title.content)\n",
    "            else:\n",
    "                content.extend(title[\"content\"])\n",
    "        for chapter in data.get('chapters', []):\n",
    "            if isinstance(chapter, Chapter):\n",
    "                content.extend(chapter.content)\n",
    "            else:\n",
    "                content.extend(chapter[\"content\"])\n",
    "        data['content'] = list(set(content))\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def word_count(self):\n",
    "        total = sum(chapter.word_count for chapter in self.chapters)\n",
    "        if self.title:\n",
    "            total += self.title.word_count\n",
    "        return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理对象的翻译工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chapter(chapter: Chapter, target_language: str):\n",
    "    translated_chapter = chapter.model_copy()\n",
    "    translated_chapter.language = target_language\n",
    "    translated_chapter.id = uuid4()\n",
    "    new_sentences = []\n",
    "    for s in chapter.sentences:\n",
    "        ns = s.model_copy()\n",
    "        ns.language = target_language\n",
    "        new_sentences.append(ns)\n",
    "    translated_chapter.sentences = new_sentences\n",
    "    return translated_chapter\n",
    "\n",
    "def translate_text(text_obj: TextBase, target_language: str):\n",
    "    \"\"\"\n",
    "    翻译文本对象到指定语言，如果字数超过最大长度则不进行翻译。\n",
    "    返回翻译后的对象或原对象（如果未翻译）。\n",
    "    \"\"\"\n",
    "    max_length = 256\n",
    "    if text_obj.word_count > max_length:\n",
    "        print(f\"翻译失败：{text_obj.id} 的字数超过了最大限制 {max_length}\")\n",
    "        return text_obj\n",
    "\n",
    "    translated_obj = text_obj.model_copy()\n",
    "    translated_obj.language = target_language\n",
    "    translated_obj.id = uuid4()\n",
    "    has_title = getattr(translated_obj, \"title\", None)\n",
    "    if has_title:\n",
    "        translated_obj.title.language = target_language\n",
    "        translated_obj.title.id = uuid4()\n",
    "    if isinstance(translated_obj, Article):\n",
    "        #  是文章类型，需要将所有章节翻译\n",
    "        translated_chaps = []\n",
    "        for chap in translated_obj.chapters:\n",
    "            new_chap = translate_chapter(chap, target_language)\n",
    "            translated_chaps.append(new_chap)\n",
    "        translated_obj.chapters = translated_chaps\n",
    "    if isinstance(translated_obj, Chapter):\n",
    "        # 是章节类型，直接翻译\n",
    "        translated_obj = translate_chapter(text_obj, target_language)\n",
    "    return translated_obj\n",
    "\n",
    "def sentiment_analysis_article(article: Article):\n",
    "    # 只能处理特定语言的情感\n",
    "    if article.language == \"en\":\n",
    "        return article.sentiment\n",
    "    else:\n",
    "        return \"pos\"\n",
    "    \n",
    "def sentiment_analysis_chapter(chapter: Chapter):\n",
    "    # 只能处理特定语言的情感\n",
    "    if chapter.language == \"en\":\n",
    "        return chapter.sentiment\n",
    "    else:\n",
    "        return \"pos\"\n",
    "\n",
    "def sentiment_analysis_sentence(sentence: Sentence):\n",
    "    # 只能处理特定语言的情感\n",
    "    if sentence.language == \"en\":\n",
    "        return sentence.sentiment\n",
    "    else:\n",
    "        return \"pos\"\n",
    "\n",
    "def aspect_based_sentiment_analysis(sentence: Sentence):\n",
    "    if sentence.language == \"en\":\n",
    "        return sentence.aspect_based_sent\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_title(article: Article):\n",
    "    return article.title\n",
    "\n",
    "def split_article(article: Article):\n",
    "    return article.chapters\n",
    "\n",
    "def split_chapter(chapter: Chapter):\n",
    "    return chapter.sentences\n",
    "\n",
    "def construct_chapter(sentences: List[Sentence]):\n",
    "    languages = []\n",
    "    for sent in sentences:\n",
    "        languages.append(sent.language)\n",
    "    if len(set(languages)) == 1:\n",
    "        # 如果只有一种语言，就设置为这种语言\n",
    "        chap_lang = languages[0]\n",
    "    else:\n",
    "        chap_lang = \"mix_language\"\n",
    "    chapter = Chapter(sentences=sentences, language=chap_lang)\n",
    "    return chapter\n",
    "\n",
    "def construct_article(chapters: List[Chapter], title=None):\n",
    "    languages = []\n",
    "    for chap in chapters:\n",
    "        languages.append(chap.language)\n",
    "    if title:\n",
    "        languages.append(title.language)\n",
    "    if len(set(languages)) == 1:\n",
    "        # 如果只有一种语言，就设置为这种语言\n",
    "        art_lang = languages[0]\n",
    "    else:\n",
    "        art_lang = \"mix_language\"\n",
    "    article = Article(chapters=chapters, language=art_lang, title=title)\n",
    "    return article\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 包装好的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "\n",
    "OBJ_DIR = \"./data\"\n",
    "\n",
    "def save_object_to_json(obj: BaseModel):\n",
    "    \"\"\"\n",
    "    将给定的对象保存为JSON文件。\n",
    "    :param obj: 要保存的对象，可以是Sentence, Chapter, 或Article类型的实例。\n",
    "    \"\"\"\n",
    "    # 确保目录存在\n",
    "    Path(OBJ_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 使用对象的id属性作为文件名\n",
    "    file_path = Path(OBJ_DIR) / f\"{obj.id}.json\"\n",
    "    \n",
    "    # 序列化为JSON字符串\n",
    "    json_str = obj.model_dump_json()\n",
    "    \n",
    "    # 写入文件\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(json_str)\n",
    "\n",
    "def load_object_from_json(obj_id: UUID):\n",
    "    \"\"\"\n",
    "    从JSON文件加载对象，并自动转换为正确的Pydantic模型。\n",
    "    :param obj_id: 对象的uuid\n",
    "    :return: 返回对应的Sentence, Chapter, 或Article类型的实例。\n",
    "    \"\"\"\n",
    "    # 确保目录存在\n",
    "    Path(OBJ_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 使用对象的id属性作为文件名\n",
    "    file_path = Path(OBJ_DIR) / f\"{obj_id}.json\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # 根据\"data\"中的特定字段判断对象类型\n",
    "    if 'chapters' in data:\n",
    "        return Article.model_validate(data)\n",
    "    elif 'sentences' in data:\n",
    "        return Chapter.model_validate(data)\n",
    "    elif 'aspect_based_sent' in data:\n",
    "        return Sentence.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\"无法识别JSON数据所属的模型类型\")\n",
    "\n",
    "# # 示例用法：\n",
    "# sentence = Sentence(content=[uuid4()], language=\"fr\", sentiment=\"pos\")\n",
    "# save_object_to_json(sentence)\n",
    "# loaded_sentence = load_object_from_json(sentence.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tool(text_obj_id: UUID, target_language: str):\n",
    "    text_obj = load_object_from_json(text_obj_id)\n",
    "    translated_text = translate_text(text_obj, target_language)\n",
    "    save_object_to_json(translated_text)\n",
    "    return translated_text.id\n",
    "\n",
    "def sentiment_analysis_article_tool(article_id: UUID):\n",
    "    article_obj = load_object_from_json(article_id)\n",
    "    senti = sentiment_analysis_article(article_obj)\n",
    "    return senti\n",
    "\n",
    "    \n",
    "def sentiment_analysis_chapter_tool(chapter_id: UUID):\n",
    "    chapter_obj = load_object_from_json(chapter_id)\n",
    "    senti = sentiment_analysis_article(chapter_obj)\n",
    "    return senti\n",
    "\n",
    "def sentiment_analysis_sentence_tool(sentence_id: UUID):\n",
    "    sentence_obj = load_object_from_json(sentence_id)\n",
    "    senti = sentiment_analysis_article(sentence_obj)\n",
    "    return senti\n",
    "\n",
    "def aspect_based_sentiment_analysis_tool(sentence_id: UUID):\n",
    "    sentence_obj = load_object_from_json(sentence_id)\n",
    "    senti = aspect_based_sentiment_analysis(sentence_obj)\n",
    "    return senti\n",
    "\n",
    "def get_title_tool(sentence_id: UUID):\n",
    "    sentence_obj = load_object_from_json(sentence_id)\n",
    "    return get_title(sentence_obj)\n",
    "\n",
    "def split_article_tool(article_id: UUID):\n",
    "    article_obj = load_object_from_json(article_id)\n",
    "    chap_objs = split_article(article_obj)\n",
    "    chap_ids = [chap.id for chap in chap_objs]\n",
    "    return chap_ids\n",
    "\n",
    "def split_chapter_tool(chapter_id: UUID):\n",
    "    chapter_obj = load_object_from_json(chapter_id)\n",
    "    sent_objs = split_article(chapter_obj)\n",
    "    sent_ids = [sent.id for sent in sent_objs]\n",
    "    return sent_ids\n",
    "\n",
    "def construct_chapter_tool(sentences_id: List[UUID]):\n",
    "    sent_objs = [load_object_from_json(s) for s in sentences_id]\n",
    "    chapter_obj = construct_chapter(sent_objs)\n",
    "    save_object_to_json(chapter_obj)\n",
    "    return chapter_obj.id\n",
    "\n",
    "def construct_article_tool(chapters_id: List[UUID], title_id=None):\n",
    "    chap_objs = [load_object_from_json(c) for c in chapters_id]\n",
    "    if title_id:\n",
    "        title_obj = load_object_from_json(title_id)\n",
    "        article_obj = construct_article(chap_objs, title=title_obj)\n",
    "    else:\n",
    "        article_obj = construct_article(chap_objs)\n",
    "    save_object_to_json(article_obj)\n",
    "    return article_obj.id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建句子\n",
    "sentence1 = Sentence(\n",
    "    content=[uuid4(), uuid4(), uuid4()],\n",
    "    language=\"zh\",\n",
    "    sentiment=\"pos\"  # 可手动指定情感\n",
    ")\n",
    "\n",
    "\n",
    "asp = Aspect_Sentiment(aspect_term=\"s1\", opinion=\"o1\", sentiment=\"pos\")\n",
    "sentence2 = Sentence(\n",
    "    content=[uuid4(), uuid4()],\n",
    "    language=\"zh\",  # 情感自动生成\n",
    "    aspect_based_sent=[asp]\n",
    ")\n",
    "\n",
    "# 创建章节（自动聚合内容）\n",
    "chapter = Chapter(\n",
    "    sentences=[sentence1, sentence2],\n",
    "    language=\"zh\"\n",
    ")\n",
    "cid = chapter.id\n",
    "save_object_to_json(chapter)\n",
    "chapter = load_object_from_json(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双向翻译测试通过 ✅\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体工具调用通过json读写来完成，id就是那个唯一的名称，通过对id处理读取对应的json，然后每次输出如果返回的是对象或者对象的列表，就用json文件的id取代.\n",
    "这里id代表的就是这个文件的全文的意思\n",
    "\n",
    "\n",
    "接下来的步骤：\n",
    "1. 把文本这块的环境搭建好\n",
    "2. 构建一些评测例子\n",
    "3. 搭建baseline，测试当前模型的效果（考虑qwen7b，gpt4o）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建评测任务\n",
    "思路：评测任务需要给定的输入到目标的输出,这里面的id就是唯一对应的文本的内容\n",
    "\n",
    "文件操作的话沟通交流的时候就是文件所在路径，通过文件路径对应到实际操作的那个文件对象\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网页获取类\n",
    "\n",
    "新闻网站：\n",
    "api1：查询某日的新闻列表，获取新闻题目对应的题目\n",
    "api2: 根据新闻的url获取新闻的全文\n",
    "api3: 输入关键词，获取关于这个关键词的所有新闻，关键词设置一个新的类，\n",
    "\n",
    "统计数据网站：\n",
    "世界银行\n",
    "\n",
    "直接考虑拿现有的工具数据集去玩\n",
    "\n",
    "以及从现有的工具集里面找一些改造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ToolBeHonest就是我要的那个' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mToolBeHonest就是我要的那个\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ToolBeHonest就是我要的那个' is not defined"
     ]
    }
   ],
   "source": [
    "ToolBeHonest就是我要的那个"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
